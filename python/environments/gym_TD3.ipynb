{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2115635c",
   "metadata": {},
   "source": [
    "# Explore the Nav2D environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco as mj\n",
    "import gymnasium as gym\n",
    "import nav2d\n",
    "import pyautogui\n",
    "\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "\n",
    "# Reference for setting visual flags https://mujoco.readthedocs.io/en/stable/APIreference/APItypes.html#mjtvisflag\n",
    "visual_options = {2: True, 8: True}      # e.g., visualize the joints by setting mjVIS_JOINT (index 2) = True\n",
    "\n",
    "# There are a few visualization things that cannot be set when making the env\n",
    "# Ref - https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/mujoco_rendering.py\n",
    "# They can only be changed via keypresses in the gymnasium mujoco rendering. \n",
    "# Here are some flags to specify what key to press. The key presses are simulated using `pyautogui`\n",
    "# (Super rough appproach but oh well)\n",
    "DEFAULT_CAMERA = \"overhead_camera\"\n",
    "ENABLE_FRAME = True                     # enable the body frames\n",
    "RENDER_EVERY_FRAME = True              # similar sim speed as MuJoCo rendering when set to False, else slower\n",
    "\n",
    "env = gym.make(\"Nav2D-v0\", \n",
    "               render_mode=\"human\", \n",
    "               width=width, height=height,\n",
    "               default_camera_config=default_camera_config,\n",
    "               visual_options=visual_options\n",
    "               )\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Simulate keypress for visualization elements in gymnasium MuJoCo rendering\n",
    "if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "if ENABLE_FRAME: pyautogui.press('e') \n",
    "if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "obs_hist = []\n",
    "obs_hist.append(obs)\n",
    "for i in range(5):\n",
    "    done = False\n",
    "    # enable the body frame by simulating a keypress once lmao\n",
    "    \n",
    "    while not done:\n",
    "        action = [1.0, 0.2]\n",
    "        nobs, rew, term, trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        obs = nobs if not done else env.reset(options={\"agent_randomize\": True, \"goal_randomize\": True})[0]\n",
    "        obs_hist.append(obs)\n",
    "        # if done: print(nobs, info)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf4d33",
   "metadata": {},
   "source": [
    "# Custom TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7431616",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- import the custom-made TD3 algorithm\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import os, re\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from algorithms import TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de3b88",
   "metadata": {},
   "source": [
    "## Custom TD3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'TD3_v0': {\n",
    "        'actor_config': [256, 256],\n",
    "        'critic_config': [256, 256]\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_NAME = 'TD3_v0'\n",
    "ALPHA1 = 1e-3\n",
    "ALPHA2 = 1e-3\n",
    "BETA = 1e-3\n",
    "GAMMA = 0.99\n",
    "TAU_C = 5e-3\n",
    "TAU_A = 5e-3\n",
    "SIGMA = 0.2\n",
    "CLIP = 0.5\n",
    "\n",
    "BUFFER_SIZE = 20_000\n",
    "BUFFER_INIT = 2_000\n",
    "BATCH_SIZE = 1024\n",
    "  \n",
    "UPDATE_FREQ = 2\n",
    "UPDATE_STEP = 2\n",
    "TRAIN_ITER = 50_000\n",
    "RAND_GOAL_FREQ = 1\n",
    "TRAIN_CRIT = {\"pass_limit\": 3, \"pass_score\": 0, 'coeff_var_limit': 1.0}\n",
    "RESULT_FOLDER = 'Nav2D_TD3_results'\n",
    "CUDA_ENABLED = True\n",
    "EARLY_STOP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850313e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TimeLimit(gym.make(\"Nav2D-v0\", render_mode=\"human\"), max_episode_steps = 1_000)\n",
    "\n",
    "env = gym.make(\"Nav2D-v0\", render_mode=None, visual_options = {2: True, 8: True})\n",
    "for i in range(1):    \n",
    "    seed = np.random.randint(1,100)\n",
    "    TD3_experiment = TD3(model_name = MODEL_NAME, model_registry=model_registry, env=env,\n",
    "                     alpha1=ALPHA1,alpha2=ALPHA2,beta=BETA,gamma=GAMMA,\n",
    "                     tau_c=TAU_C,tau_a=TAU_A,sigma=SIGMA,clip=CLIP,\n",
    "                     buffer_size=BUFFER_SIZE,buffer_init=BUFFER_INIT, batch_size=BATCH_SIZE, \n",
    "                     update_f=UPDATE_FREQ, update_step=UPDATE_STEP, iter=TRAIN_ITER,\n",
    "                     rand_goal_freq=RAND_GOAL_FREQ,\n",
    "                     seed=seed,\n",
    "                     train_crit=TRAIN_CRIT,\n",
    "                     result_folder=RESULT_FOLDER,\n",
    "                     cuda_enabled=CUDA_ENABLED)                 \n",
    "    TD3_experiment.train(early_stop=EARLY_STOP,verbose=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d969271",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5b1de",
   "metadata": {},
   "source": [
    "## Load and Simulate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff17945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def EMA_filter(reward: list, alpha: float=0.1):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "def plot_fn(history, xlabel:str='step', ylabel: str='reward', alpha: float=0.0):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = EMA_filter(history, alpha)\n",
    "\n",
    "        legend = []\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, history[:n_episodes], color = \"blue\"); legend.append(ylabel)\n",
    "        if alpha:\n",
    "            plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\"); legend.append('filtered '+ylabel)\n",
    "        # plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(legend)\n",
    "        plt.grid(which='both')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # if self.save_path:\n",
    "        #     plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int,\n",
    "                 act_dim: int,\n",
    "                 act_low, act_high, \n",
    "                 hidden_layers=[64,64], \n",
    "                 cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_low, self.act_high = act_low, act_high\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        # hidden layers\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(obs_dim, layer_size))\n",
    "            obs_dim = layer_size\n",
    "        # output layers\n",
    "        self.layers.append(nn.Linear(obs_dim, self.act_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        act_tanh = self.tanh(output)\n",
    "\n",
    "        # rescale to environment bounds\n",
    "        act = 0.5 * (act_tanh + 1.0) * (self.act_high - self.act_low) + self.act_low\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97437866",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually select a folder/run to load\n",
    "run_number = 'run_00016'\n",
    "\n",
    "## Find the paths to the param_config and model checkpoint\n",
    "# RESULT_DIR = os.path.dirname(REINFORCE_experiment.save_path)    # Can run this after running one experiment\n",
    "BASE_DIR = os.getcwd()\n",
    "RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "## Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_name = data['parameters']['model_name']\n",
    "model_config = model_registry[model_name]['actor_config']\n",
    "\n",
    "n_test_eps = 2\n",
    "\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "render_mode = 'human' if n_test_eps <= 5 else 'rgb_array'\n",
    "\n",
    "env_test = gym.make('Nav2D-v0',\n",
    "               render_mode=render_mode,\n",
    "               width=width,height=height,\n",
    "               default_camera_config=default_camera_config\n",
    "            )\n",
    "\n",
    "obs_space = env_test.observation_space.shape[0]\n",
    "act_space = env_test.action_space.shape[0]\n",
    "act_low = torch.as_tensor(env_test.action_space.low, dtype=torch.float32, device = 'cpu')\n",
    "act_high = torch.as_tensor(env_test.action_space.high, dtype=torch.float32, device = 'cpu')\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "policy_net= Actor(obs_space, act_space, \n",
    "                    act_low=act_low, act_high=act_high, \n",
    "                    hidden_layers=model_config)\n",
    "load_model(policy_net, MODEL_PATH)\n",
    "\n",
    "act_hist = []\n",
    "rew_hist = []\n",
    "eps_rew_hist = []\n",
    "torso_z_hist = []\n",
    "\n",
    "noise_std = 0\n",
    "truncated = True\n",
    "\n",
    "for eps in range(n_test_eps):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    torso_z_hist.append(obs[0])\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.as_tensor(obs, dtype = torch.float32, device = 'cpu')\n",
    "            act = policy_net(obs_t)\n",
    "            act += torch.randn_like(act) * noise_std\n",
    "\n",
    "            nobs, rew, term, trunc, _ = env_test.step(act.numpy())\n",
    "\n",
    "            act_hist.append(act.numpy())\n",
    "            torso_z_hist.append(obs[0])\n",
    "\n",
    "            # rew_hist.append(rew)\n",
    "            if truncated:\n",
    "                rew_hist.append(rew)\n",
    "                truncated = False\n",
    "            else:\n",
    "                rew_hist.append(rew_hist[-1]+rew)\n",
    "\n",
    "            done = term or trunc\n",
    "            if not done:\n",
    "                obs = nobs  \n",
    "            else:\n",
    "                obs = env_test.reset()[0]\n",
    "                truncated = True\n",
    "                eps_rew_hist.append(rew_hist[-1])\n",
    "                msg = 'done due to termination' if term else 'done due to truncation'\n",
    "    # print(f'episode {eps:3d} ' + msg)\n",
    "env_test.close()\n",
    "\n",
    "plot_fn(rew_hist, ylabel='instantaneous reward')\n",
    "\n",
    "# Statistical analysis\n",
    "if n_test_eps > 1:\n",
    "    print(f\"Evaluation of {run_number} after {n_test_eps} eval episodes | µ = {mean(eps_rew_hist):5.2f} | σ = {stdev(eps_rew_hist):5.2f}\")\n",
    "\n",
    "else:\n",
    "    # --- plot the z-position of the torso\n",
    "    plot_fn(torso_z_hist, ylabel='torso z level')\n",
    "\n",
    "    # --- unpack the actions and plot\n",
    "    act1_hist, act2_hist, act3_hist, act4_hist, act5_hist, act6_hist = zip(*act_hist)\n",
    "    legends=[]\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(range(len(act1_hist)), act1_hist, color = \"blue\"); legends.append(r\"right thigh $\\tau_{0}$\")\n",
    "    plt.plot(range(len(act2_hist)), act2_hist, color = \"orange\"); legends.append(r\"right leg $\\tau_{1}$\")\n",
    "    plt.plot(range(len(act3_hist)), act3_hist, color = \"green\"); legends.append(r\"right foot $\\tau_{2}$\")\n",
    "    plt.title(\"Right leg torques vs time\")\n",
    "    plt.legend(legends, bbox_to_anchor=[0.5, -0.1], loc='center', ncol=10)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "\n",
    "    legends=[]\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(range(len(act4_hist)), act4_hist, color = \"blue\"); legends.append(r\"left thigh $\\tau_{3}$\")\n",
    "    plt.plot(range(len(act5_hist)), act5_hist, color = \"orange\"); legends.append(r\"left leg $\\tau_{l2}$\")\n",
    "    plt.plot(range(len(act6_hist)), act6_hist, color = \"green\"); legends.append(r\"left foot $\\tau_{l3}$\")\n",
    "    plt.title(\"Left leg torques vs time\")\n",
    "    plt.legend(legends, bbox_to_anchor=[0.5, -0.1], loc='center', ncol=10)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fe87d",
   "metadata": {},
   "source": [
    "# SB3 TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0f51c",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c03c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3,SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import numpy as np\n",
    "import os, re, json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf9c29",
   "metadata": {},
   "source": [
    "Function to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90905585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the evaluation function:\n",
    "def eval(env: gym.Env, \n",
    "         num_evals: int, \n",
    "         model):\n",
    "    # reward list:\n",
    "    eval_rew_hist = []\n",
    "\n",
    "    # for each episode in the num_evals:\n",
    "    for _ in range(num_evals):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # initialize episodic reward:\n",
    "        eval_rew = 0\n",
    "\n",
    "        # while False:\n",
    "        while not done:\n",
    "            # get action and step:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            \n",
    "            # advance reward:\n",
    "            eval_rew += reward\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "    \n",
    "        # append:\n",
    "        eval_rew_hist.append(eval_rew)\n",
    "\n",
    "    return np.mean(eval_rew_hist).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70826bdc",
   "metadata": {},
   "source": [
    "## SB3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a163139",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "buffer_size=10_000\n",
    "learning_starts=1_000\n",
    "batch_size=512\n",
    "tau=5e-3\n",
    "gamma=0.99\n",
    "train_freq=1\n",
    "gradient_steps=4\n",
    "action_noise=None\n",
    "n_steps=1\n",
    "policy_delay=2\n",
    "target_policy_noise=0.3\n",
    "target_noise_clip=0.5\n",
    "verbose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53751a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"Nav2D-v0\", n_envs=16, vec_env_cls=DummyVecEnv)\n",
    "# env = gym.make(\"Nav2D-v0\", render_mode=\"human\", max_episode_steps=2_000, visual_options={2:True})\n",
    "\n",
    "# evaluation environment:\n",
    "eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = 1000, render_mode = \"rgb_array\")\n",
    "\n",
    "# # wrapper to rescale the action of the environment\n",
    "# action_bounds = np.array([1.0, 0.0001, 1.0], dtype=np.float32)\n",
    "# vec_env = RescaleAction(vec_env, min_action=-action_bounds, max_action=action_bounds)\n",
    "\n",
    "# custom MlpPolicy\n",
    "model = TD3(\"MlpPolicy\", env, \n",
    "            learning_rate=learning_rate,         # lr for all networds - Q-values, Actor, Value function\n",
    "            buffer_size=buffer_size,         # replay buffer size\n",
    "            learning_starts=learning_starts,        # # of data collection step before training\n",
    "            batch_size=batch_size,\n",
    "            tau=tau,                   # polyak update coefficient\n",
    "            gamma=gamma,\n",
    "            train_freq=train_freq,\n",
    "            gradient_steps=gradient_steps, \n",
    "            action_noise=action_noise, \n",
    "            n_steps=n_steps,                  # n-step TD learning\n",
    "            policy_delay=policy_delay,             # the policy and target networks are updated every policy_delay steps\n",
    "            target_policy_noise=target_policy_noise,    # stdev of noise added to target policy\n",
    "            target_noise_clip=target_noise_clip,      # limit of asbsolute value of noise\n",
    "            verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481556a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                    \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[0m/home/controlslab/ROS2_DRL_Navigation/.venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/controlslab/ROS2_DRL_Navigation/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path '/home/controlslab/ROS2_DRL_Navigation/python/environments/Nav2D_TD3_SB3_results/result_3' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n",
      "training progress:   3%|\u001b[38;2;51;255;0m█▎                                          \u001b[0m| 3/100 [01:08<37:12, 23.01s/it]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run parameters:\n",
    "number_of_runs = 100\n",
    "steps_per_run = 25000\n",
    "num_evals = 10\n",
    "\n",
    "# initialize the total reward:\n",
    "total_reward = []\n",
    "\n",
    "# model saving parameters:\n",
    "base_path = os.path.join(os.getcwd(), \"Nav2D_TD3_SB3_results\")\n",
    "result_number = f\"result_{len(os.listdir(base_path)) + 1}\"\n",
    "results_path = os.path.join(base_path, result_number)\n",
    "\n",
    "# using model.learn approach:\n",
    "for run in tqdm(range(number_of_runs), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "    # learn every run:\n",
    "    model.learn(total_timesteps = steps_per_run, reset_num_timesteps = False)\n",
    "\n",
    "    # after learning:\n",
    "    eval_reward = eval(eval_env, num_evals = num_evals, model = model)\n",
    "\n",
    "    # append the eval reward to the total reward:\n",
    "    total_reward.append(eval_reward)\n",
    "\n",
    "    # make a directory for this run:\n",
    "    model.save(os.path.join(results_path, f\"run_{run+1}\"))\n",
    "\n",
    "# close environment when done:\n",
    "env.close()\n",
    "\n",
    "# Save the result-params mapping into a json file\n",
    "trial_to_param_path = os.path.join(base_path,'trial_to_param.json')\n",
    "if os.path.exists(trial_to_param_path):\n",
    "    with open(trial_to_param_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = {result_number: \"\"}\n",
    "\n",
    "hyperparam_codified = f\"{learning_rate}_{buffer_size}_{learning_starts}_{batch_size}_{tau}_{gamma}\"\n",
    "hyperparam_codified += f\"{train_freq}_{gradient_steps}_{n_steps}_{policy_delay}_{target_policy_noise}_{target_noise_clip}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "data[result_number] = hyperparam_codified_time\n",
    "\n",
    "with open(trial_to_param_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caf7a5",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "RESULT_FOLDER = 'Nav2D_TD3_SB3_results'\n",
    "RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.exists(os.path.join(RESULT_DIR,d))]\n",
    "run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "# model.save('reacher')\n",
    "\n",
    "# trial_number = max(run_numbers, default=-1) + 1\n",
    "# model.save(f'{RESULT_FOLDER}/run_{trial_number:05d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464fa1d",
   "metadata": {},
   "source": [
    "## Load and Simulate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "\n",
    "model_load = TD3.load('Nav2D_TD3_SB3_results/run_00006')\n",
    "\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "camera_id = 2\n",
    "\n",
    "DEFAULT_CAMERA = \"overhead_camera\"\n",
    "ENABLE_FRAME = True                     # enable the body frames\n",
    "RENDER_EVERY_FRAME = True              # similar sim speed as MuJoCo rendering when set to False, else slower\n",
    "\n",
    "test_env = gym.make(\"Nav2D-v0\", render_mode='human', \n",
    "                    width=width,height=height,\n",
    "                    default_camera_config=default_camera_config,\n",
    "                    camera_id=camera_id,\n",
    "                    # frame_skip=2,\n",
    "                    # camera_name=\"camera\",\n",
    "                    max_episode_steps=1_000\n",
    "                    )\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "if ENABLE_FRAME: pyautogui.press('e') \n",
    "if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "for eps in range(5):\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model_load.predict(obs, deterministic=True)\n",
    "        nobs, rew, term, trunc, _ = test_env.step(action)\n",
    "        done = term or trunc\n",
    "        obs = nobs if not done else test_env.reset()[0]\n",
    "        # vec_env.render(\"human\")\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda143af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
