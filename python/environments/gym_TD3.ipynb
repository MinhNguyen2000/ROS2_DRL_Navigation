{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2115635c",
   "metadata": {},
   "source": [
    "# Explore the Nav2D environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco as mj\n",
    "import gymnasium as gym\n",
    "import nav2d\n",
    "import pyautogui\n",
    "\n",
    "# TODO - test the rendering in \"rgb_array\"\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "\n",
    "# Reference for setting visual flags https://mujoco.readthedocs.io/en/stable/APIreference/APItypes.html#mjtvisflag\n",
    "visual_options = {2: True, 8: True}      # e.g., visualize the joints by setting mjVIS_JOINT (index 2) = True\n",
    "\n",
    "# There are a few visualization things that cannot be set when making the env\n",
    "# Ref - https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/mujoco_rendering.py\n",
    "# They can only be changed via keypresses in the gymnasium mujoco rendering. \n",
    "# Here are some flags to specify what key to press. The key presses are simulated using `pyautogui`\n",
    "# (Super rough appproach but oh well)\n",
    "DEFAULT_CAMERA = \"overhead_camera\"\n",
    "ENABLE_FRAME = True                     # enable the body frames\n",
    "RENDER_EVERY_FRAME = True              # similar sim speed as MuJoCo rendering when set to False, else slower\n",
    "\n",
    "env = gym.make(\"Nav2D-v0\", \n",
    "               render_mode=\"human\", \n",
    "               width=width, height=height,\n",
    "               default_camera_config=default_camera_config,\n",
    "               visual_options=visual_options\n",
    "               )\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Simulate keypress for visualization elements in gymnasium MuJoCo rendering\n",
    "if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "if ENABLE_FRAME: pyautogui.press('e') \n",
    "if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "# TODO - test resetting with the randomize flags\n",
    "# TODO - test reward (esp when terminated due to 1. goal and 2. obstacle)\n",
    "for i in range(1):\n",
    "    done = False\n",
    "    # enable the body frame by simulating a keypress once lmao\n",
    "    \n",
    "    while not done:\n",
    "        # TODO - when setting the action as env.action_space.sample, the motion is very slow. Need to find a good action_space bound in nav2d.py\n",
    "        action = [1.0, 0, 1.0]\n",
    "        nobs, rew, term, trunc, info = env.step(action)\n",
    "        \n",
    "        done = term or trunc\n",
    "        obs = nobs if not done else env.reset()[0]\n",
    "        # if done: print(nobs, info)\n",
    "\n",
    "# TODO - quitting does not stop very gracefully. Why?\n",
    "# When Esc from the rendering window, it says \"Pressed ESC. Quitting.\" but never finish\n",
    "# Stopping from the notebook throw a huge KeyboardInterrupt error\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de3b88",
   "metadata": {},
   "source": [
    "# Custom TD3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfaaf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- import the custom-made TD3 algorithm\n",
    "import gymnasium as gym\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import numpy as np\n",
    "import os, re\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from algorithms import TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'TD3_v0': {\n",
    "        'actor_config': [256, 256],\n",
    "        'critic_config': [256, 256]\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_NAME = 'TD3_v0'\n",
    "ALPHA1 = 1e-3\n",
    "ALPHA2 = 1e-3\n",
    "BETA = 1e-3\n",
    "GAMMA = 0.99\n",
    "TAU_C = 5e-3\n",
    "TAU_A = 5e-3\n",
    "SIGMA = 0.2\n",
    "CLIP = 0.5\n",
    "\n",
    "BUFFER_SIZE = 10_000\n",
    "BUFFER_INIT = 1_000\n",
    "BATCH_SIZE = 512\n",
    "  \n",
    "UPDATE_FREQ = 2\n",
    "UPDATE_STEP = 2\n",
    "TRAIN_ITER = 100_000\n",
    "TRAIN_CRIT = {\"pass_limit\": 3, \"pass_score\": -10, 'coeff_var_limit': 1.0}\n",
    "RESULT_FOLDER = 'Nav2D_TD3_results'\n",
    "CUDA_ENABLED = True\n",
    "EARLY_STOP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850313e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   1%|\u001b[38;2;46;111;64m▎                                            \u001b[0m| 560/100000 [00:11<2:09:38, 12.78it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    0 with reward of 728.220. Evaluation results μ=-202.567, σ=59.582, CV= 0.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   1%|\u001b[38;2;46;111;64m▌                                             \u001b[0m| 1119/100000 [00:17<50:06, 32.89it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    1 with reward of 1774.696. Evaluation results μ=1844.106, σ=44.054, CV= 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   2%|\u001b[38;2;46;111;64m▊                                             \u001b[0m| 1643/100000 [00:22<50:36, 32.39it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    2 with reward of 1848.317. Evaluation results μ=1858.790, σ=51.306, CV= 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   2%|\u001b[38;2;46;111;64m█                                             \u001b[0m| 2198/100000 [00:28<49:25, 32.98it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    3 with reward of 1790.798. Evaluation results μ=1847.488, σ=48.958, CV= 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   3%|\u001b[38;2;46;111;64m█▏                                            \u001b[0m| 2707/100000 [00:34<53:19, 30.41it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    4 with reward of 1849.939. Evaluation results μ=1847.059, σ=48.377, CV= 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   3%|\u001b[38;2;46;111;64m█▌                                            \u001b[0m| 3274/100000 [00:39<53:36, 30.07it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    5 with reward of 1829.112. Evaluation results μ=1881.734, σ=48.458, CV= 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   4%|\u001b[38;2;46;111;64m█▋                                          \u001b[0m| 3810/100000 [00:45<1:09:54, 22.93it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    6 with reward of 1836.558. Evaluation results μ=1853.905, σ=53.680, CV= 0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   4%|\u001b[38;2;46;111;64m█▉                                          \u001b[0m| 4340/100000 [00:51<1:09:26, 22.96it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    7 with reward of 1872.901. Evaluation results μ=1867.525, σ=47.499, CV= 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   5%|\u001b[38;2;46;111;64m██▎                                           \u001b[0m| 4894/100000 [00:57<53:13, 29.78it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    8 with reward of 1881.086. Evaluation results μ=1828.246, σ=41.726, CV= 0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   5%|\u001b[38;2;46;111;64m██▌                                           \u001b[0m| 5449/100000 [01:03<50:31, 31.19it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode    9 with reward of 1807.877. Evaluation results μ=1842.286, σ=46.888, CV= 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   6%|\u001b[38;2;46;111;64m██▊                                           \u001b[0m| 5987/100000 [01:08<48:32, 32.27it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   10 with reward of 1843.657. Evaluation results μ=1842.375, σ=44.877, CV= 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   7%|\u001b[38;2;46;111;64m██▉                                           \u001b[0m| 6520/100000 [01:13<54:18, 28.68it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   11 with reward of 1873.170. Evaluation results μ=1866.524, σ=56.542, CV= 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   7%|\u001b[38;2;46;111;64m███▎                                          \u001b[0m| 7075/100000 [01:19<43:04, 35.95it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   12 with reward of 1880.577. Evaluation results μ=1868.379, σ=50.601, CV= 0.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   8%|\u001b[38;2;46;111;64m███▌                                          \u001b[0m| 7621/100000 [01:25<50:32, 30.47it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   13 with reward of 1767.724. Evaluation results μ=1835.451, σ=40.721, CV= 0.022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   8%|\u001b[38;2;46;111;64m███▊                                          \u001b[0m| 8170/100000 [01:31<51:03, 29.98it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   14 with reward of 1782.939. Evaluation results μ=1850.162, σ=39.324, CV= 0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   9%|\u001b[38;2;46;111;64m████                                          \u001b[0m| 8712/100000 [01:36<52:21, 29.06it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   15 with reward of 1867.613. Evaluation results μ=1856.829, σ=56.080, CV= 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:   9%|\u001b[38;2;46;111;64m████▎                                         \u001b[0m| 9264/100000 [01:42<49:58, 30.26it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   16 with reward of 1855.215. Evaluation results μ=1862.158, σ=52.471, CV= 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:  10%|\u001b[38;2;46;111;64m████▌                                         \u001b[0m| 9815/100000 [01:48<49:41, 30.25it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   17 with reward of 1812.580. Evaluation results μ=1846.688, σ=46.125, CV= 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:  10%|\u001b[38;2;46;111;64m████▋                                        \u001b[0m| 10335/100000 [01:54<49:53, 29.95it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good training at episode   18 with reward of 1881.030. Evaluation results μ=1850.014, σ=55.288, CV= 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_00007:  10%|\u001b[38;2;46;111;64m████▋                                        \u001b[0m| 10451/100000 [01:54<15:36, 95.65it/s]\u001b[0m"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Nav2D-v0\", render_mode=\"human\")\n",
    "\n",
    "for i in range(1):    \n",
    "    seed = np.random.randint(1,100)\n",
    "    TD3_experiment = TD3(model_name = MODEL_NAME, model_registry=model_registry, env=env,\n",
    "                     alpha1=ALPHA1,alpha2=ALPHA2,beta=BETA,gamma=GAMMA,\n",
    "                     tau_c=TAU_C,tau_a=TAU_A,sigma=SIGMA,clip=CLIP,\n",
    "                     buffer_size=BUFFER_SIZE,buffer_init=BUFFER_INIT, batch_size=BATCH_SIZE, \n",
    "                     update_f=UPDATE_FREQ, update_step=UPDATE_STEP, iter=TRAIN_ITER,\n",
    "                     seed=seed,\n",
    "                     train_crit=TRAIN_CRIT,\n",
    "                     result_folder=RESULT_FOLDER,\n",
    "                     cuda_enabled=CUDA_ENABLED)                 \n",
    "    TD3_experiment.train(early_stop=EARLY_STOP,verbose=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD3_experiment.reward_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70826bdc",
   "metadata": {},
   "source": [
    "# SB3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53751a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"Nav2D-v0\", n_envs=8, vec_env_cls=DummyVecEnv)\n",
    "model = TD3(\"MlpPolicy\", env, \n",
    "            learning_rate=5e-3,        # lr for all networds - Q-values, Actor, Value function\n",
    "            buffer_size=10_000,      # replay buffer size\n",
    "            learning_starts=1_000,        # # of data collection step before training\n",
    "            batch_size=1_000,\n",
    "            tau=5e-3,                  # polyak update coefficient\n",
    "            gamma=0.99,\n",
    "            train_freq=1,\n",
    "            gradient_steps=4, \n",
    "            action_noise=None, \n",
    "            n_steps=1,                  # n-step TD learning\n",
    "            policy_delay=2,             # the policy and target networks are updated every policy_delay steps\n",
    "            target_policy_noise=0.05,   # stdev of noise added to target policy\n",
    "            target_noise_clip=0.1,      # limit of asbsolute value of noise\n",
    "            verbose=2)\n",
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caf7a5",
   "metadata": {},
   "source": [
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "RESULT_FOLDER = 'Nav2D_TD3_SB3_results'\n",
    "RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "existing_runs = [d for d in os.listdir(RESULT_DIR) if os.path.exists(os.path.join(RESULT_DIR,d))]\n",
    "run_numbers = [int(re.search(r'run_(\\d{5})',d).group(1)) for d in existing_runs if re.match(r'run_\\d{5}',d)]\n",
    "# model.save('reacher')\n",
    "\n",
    "trial_number = max(run_numbers, default=-1) + 1\n",
    "model.save(f'{RESULT_FOLDER}/run_{trial_number:05d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464fa1d",
   "metadata": {},
   "source": [
    "# Load and Simulate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "\n",
    "model_load = TD3.load('Nav2D_TD3_SB3_results/run_00005')\n",
    "\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "camera_id = 2\n",
    "\n",
    "DEFAULT_CAMERA = \"overhead_camera\"\n",
    "ENABLE_FRAME = True                     # enable the body frames\n",
    "RENDER_EVERY_FRAME = True              # similar sim speed as MuJoCo rendering when set to False, else slower\n",
    "\n",
    "test_env = gym.make(\"Nav2D-v0\", render_mode='human', \n",
    "                    width=width,height=height,\n",
    "                    default_camera_config=default_camera_config,\n",
    "                    camera_id=camera_id,\n",
    "                    # frame_skip=2,\n",
    "                    # camera_name=\"camera\",\n",
    "                    # max_episode_steps=100\n",
    "                    )\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "if ENABLE_FRAME: pyautogui.press('e') \n",
    "if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "for eps in range(5):\n",
    "    obs, _ = test_env.reset()\n",
    "    dones = False\n",
    "\n",
    "    while not dones:\n",
    "        action, _ = model_load.predict(obs, deterministic=True)\n",
    "        nobs, rewards, dones, info, _ = test_env.step(action)\n",
    "        obs = nobs if not dones else test_env.reset()\n",
    "        # vec_env.render(\"human\")\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda143af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
