{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2115635c",
   "metadata": {},
   "source": [
    "# Explore the Nav2D environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e5ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ episode 3 | (d_x, d_y): ( 0.004,-0.364) | theta:  294.731 | x_dot_local: 0.0000 | x_dot_local: -0.0000 | theta_dot: 2.5227       \r"
     ]
    }
   ],
   "source": [
    "import mujoco as mj\n",
    "import gymnasium as gym\n",
    "import nav2d\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "width = 400\n",
    "height = 400\n",
    "default_camera_config = {\"azimuth\" : 90, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "\n",
    "# Reference for setting visual flags https://mujoco.readthedocs.io/en/stable/APIreference/APItypes.html#mjtvisflag\n",
    "visual_options = {2: True, 8: True}      # e.g., visualize the joints by setting mjVIS_JOINT (index 2) = True\n",
    "\n",
    "# There are a few visualization things that cannot be set when making the env\n",
    "# Ref - https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/mujoco_rendering.py\n",
    "# They can only be changed via keypresses in the gymnasium mujoco rendering. \n",
    "# Here are some flags to specify what key to press. The key presses are simulated using `pyautogui`\n",
    "# (Super rough appproach but oh well)\n",
    "DEFAULT_CAMERA = \"overhead_camera\"\n",
    "ENABLE_FRAME = True                     # enable the body frames\n",
    "RENDER_EVERY_FRAME = True              # similar sim speed as MuJoCo rendering when set to False, else slower\n",
    "\n",
    "reward_scale= {\n",
    "    \"rew_head_scale\": 10.0,\n",
    "    \"rew_head_approach_scale\": 250.0,\n",
    "    \"rew_dist_scale\": 05.0,\n",
    "    \"rew_dist_approach_scale\": 190.0,\n",
    "    \"rew_time\": -0.25,\n",
    "    \"rew_goal_scale\": 5_000.0,\n",
    "    \"rew_obst_scale\": -1_000.0\n",
    "}\n",
    "        \n",
    "randomization_options = {\n",
    "    \"agent_freq\": 1,\n",
    "    \"goal_freq\": 1\n",
    "}\n",
    "\n",
    "env = gym.make(\"Nav2D-v0\", \n",
    "               render_mode=\"human\", \n",
    "               width=width, height=height,\n",
    "               default_camera_config=default_camera_config,\n",
    "               visual_options=visual_options,\n",
    "               reward_scale_options=reward_scale,\n",
    "               randomization_options=randomization_options\n",
    "               )\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Simulate keypress for visualization elements in gymnasium MuJoCo rendering\n",
    "if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "if ENABLE_FRAME: pyautogui.press('e') \n",
    "if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "obs_hist = []\n",
    "obs_hist.append(obs)\n",
    "for i in range(5):\n",
    "    done = False\n",
    "    # enable the body frame by simulating a keypress once lmao\n",
    "    \n",
    "    while not done:\n",
    "        action = [0.0, 1.0]\n",
    "        nobs, rew, term, trunc, info = env.step(action)\n",
    "\n",
    "        delta_x, delta_y = nobs[:2]\n",
    "        sin_theta, cos_theta = nobs[2:4]\n",
    "        theta = (np.arctan2(sin_theta, cos_theta, dtype=np.float32) / np.pi * 180) % 360\n",
    "        vel_x, vel_y, vel_z = nobs[4:7]\n",
    "        lidar_scans = nobs[7:]\n",
    "        # print(f\"(d_x,d_y) = ({delta_x:4.3f},{delta_y:4.3f})  |   theta = {theta}   |   agent_vel = ({vel_x:4.3f},{vel_y:4.3f},{vel_z:4.3f})              \", end=\"\\r\")\n",
    "        done = term or trunc\n",
    "        obs = nobs if not done else env.reset()[0]\n",
    "        # if done:\n",
    "        #     print(f\"eps: {env.unwrapped.episode_counter:4d} | goal bound: {env.unwrapped.goal_bound: 5.4f}\")\n",
    "        # if info:\n",
    "        #     print(info)\n",
    "        obs_hist.append(obs)\n",
    "        # if done: print(nobs, info) \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf4d33",
   "metadata": {},
   "source": [
    "# Custom TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.arange(0,1,0.25))\n",
    "np.linspace(0,1,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7431616",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- import the custom-made TD3 algorithm\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import os, re\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from algorithms import TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de3b88",
   "metadata": {},
   "source": [
    "## Custom TD3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = {\n",
    "    'TD3_v0': {\n",
    "        'actor_config': [256, 256],\n",
    "        'critic_config': [256, 256]\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_NAME = 'TD3_v0'\n",
    "ALPHA1 = 1e-3\n",
    "ALPHA2 = 1e-3\n",
    "BETA = 1e-3\n",
    "GAMMA = 0.99\n",
    "TAU_C = 5e-3\n",
    "TAU_A = 5e-3\n",
    "SIGMA = 0.2\n",
    "CLIP = 0.5\n",
    "\n",
    "BUFFER_SIZE = 20_000\n",
    "BUFFER_INIT = 2_000\n",
    "BATCH_SIZE = 1024\n",
    "  \n",
    "UPDATE_FREQ = 2\n",
    "UPDATE_STEP = 2\n",
    "TRAIN_ITER = 50_000\n",
    "RAND_GOAL_FREQ = 1\n",
    "TRAIN_CRIT = {\"pass_limit\": 3, \"pass_score\": 0, 'coeff_var_limit': 1.0}\n",
    "RESULT_FOLDER = 'Nav2D_TD3_results'\n",
    "CUDA_ENABLED = True\n",
    "EARLY_STOP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850313e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TimeLimit(gym.make(\"Nav2D-v0\", render_mode=\"human\"), max_episode_steps = 1_000)\n",
    "\n",
    "env = gym.make(\"Nav2D-v0\", render_mode=None, visual_options = {2: True, 8: True})\n",
    "for i in range(1):    \n",
    "    seed = np.random.randint(1,100)\n",
    "    TD3_experiment = TD3(model_name = MODEL_NAME, model_registry=model_registry, env=env,\n",
    "                     alpha1=ALPHA1,alpha2=ALPHA2,beta=BETA,gamma=GAMMA,\n",
    "                     tau_c=TAU_C,tau_a=TAU_A,sigma=SIGMA,clip=CLIP,\n",
    "                     buffer_size=BUFFER_SIZE,buffer_init=BUFFER_INIT, batch_size=BATCH_SIZE, \n",
    "                     update_f=UPDATE_FREQ, update_step=UPDATE_STEP, iter=TRAIN_ITER,\n",
    "                     rand_goal_freq=RAND_GOAL_FREQ,\n",
    "                     seed=seed,\n",
    "                     train_crit=TRAIN_CRIT,\n",
    "                     result_folder=RESULT_FOLDER,\n",
    "                     cuda_enabled=CUDA_ENABLED)                 \n",
    "    TD3_experiment.train(early_stop=EARLY_STOP,verbose=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d969271",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5b1de",
   "metadata": {},
   "source": [
    "## Load and Simulate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff17945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(q_network: nn.Module, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def EMA_filter(reward: list, alpha: float=0.1):\n",
    "        ''' Function that runs an exponential moving average filter along a datastream '''\n",
    "        output = np.zeros(len(reward)+1)\n",
    "        output[0] = reward[0]\n",
    "        for idx, item in enumerate(reward):\n",
    "            output[idx+1] = (1 - alpha) * output[idx] + alpha * item\n",
    "        \n",
    "        return output\n",
    "\n",
    "def plot_fn(history, xlabel:str='step', ylabel: str='reward', alpha: float=0.0):\n",
    "        ''' Function that plots the reward and filtered reward per episode, then saves the plot in a specified save directory'''\n",
    "        n_episodes= len(history)\n",
    "        episodes = range(n_episodes)\n",
    "        filtered_reward_hist = EMA_filter(history, alpha)\n",
    "\n",
    "        legend = []\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.plot(episodes, history[:n_episodes], color = \"blue\"); legend.append(ylabel)\n",
    "        if alpha:\n",
    "            plt.plot(episodes, filtered_reward_hist[:n_episodes], color = \"red\"); legend.append('filtered '+ylabel)\n",
    "        # plt.title(f'Total reward per episode - {self.hyperparam_config}')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(legend)\n",
    "        plt.grid(which='both')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # if self.save_path:\n",
    "        #     plt.savefig(os.path.join(self.save_path,'reward_history.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int,\n",
    "                 act_dim: int,\n",
    "                 act_low, act_high, \n",
    "                 hidden_layers=[64,64], \n",
    "                 cuda_enabled=False):\n",
    "        ''' Initialize the model and create a list of layers '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_low, self.act_high = act_low, act_high\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        # hidden layers\n",
    "        for layer_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(obs_dim, layer_size))\n",
    "            obs_dim = layer_size\n",
    "        # output layers\n",
    "        self.layers.append(nn.Linear(obs_dim, self.act_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        if cuda_enabled: self.cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers[:-1]:\n",
    "            input = torch.relu(layer(input))\n",
    "        output = self.layers[-1](input)\n",
    "        act_tanh = self.tanh(output)\n",
    "\n",
    "        # rescale to environment bounds\n",
    "        act = 0.5 * (act_tanh + 1.0) * (self.act_high - self.act_low) + self.act_low\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97437866",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually select a folder/run to load\n",
    "run_number = 'run_00016'\n",
    "\n",
    "## Find the paths to the param_config and model checkpoint\n",
    "# RESULT_DIR = os.path.dirname(REINFORCE_experiment.save_path)    # Can run this after running one experiment\n",
    "BASE_DIR = os.getcwd()\n",
    "RESULT_DIR = os.path.join(BASE_DIR, RESULT_FOLDER)\n",
    "RUN_DIR = os.path.join(RESULT_DIR, run_number)\n",
    "MODEL_PATH = os.path.join(RUN_DIR,'q_network_checkpoint.pth')\n",
    "\n",
    "## Find the model configuration\n",
    "with open(os.path.join(RUN_DIR,\"param_config.json\"),'r') as f:\n",
    "    data = json.load(f)\n",
    "model_name = data['parameters']['model_name']\n",
    "model_config = model_registry[model_name]['actor_config']\n",
    "\n",
    "n_test_eps = 2\n",
    "\n",
    "width = 1920\n",
    "height = 1080\n",
    "default_camera_config = {\"azimuth\" : 90, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "render_mode = 'human' if n_test_eps <= 5 else 'rgb_array'\n",
    "\n",
    "env_test = gym.make('Nav2D-v0',\n",
    "               render_mode=render_mode,\n",
    "               width=width,height=height,\n",
    "               default_camera_config=default_camera_config\n",
    "            )\n",
    "\n",
    "obs_space = env_test.observation_space.shape[0]\n",
    "act_space = env_test.action_space.shape[0]\n",
    "act_low = torch.as_tensor(env_test.action_space.low, dtype=torch.float32, device = 'cpu')\n",
    "act_high = torch.as_tensor(env_test.action_space.high, dtype=torch.float32, device = 'cpu')\n",
    "\n",
    "# Create the model according to the model version in the model registry\n",
    "policy_net= Actor(obs_space, act_space, \n",
    "                    act_low=act_low, act_high=act_high, \n",
    "                    hidden_layers=model_config)\n",
    "load_model(policy_net, MODEL_PATH)\n",
    "\n",
    "act_hist = []\n",
    "rew_hist = []\n",
    "eps_rew_hist = []\n",
    "torso_z_hist = []\n",
    "\n",
    "noise_std = 0\n",
    "truncated = True\n",
    "\n",
    "for eps in range(n_test_eps):\n",
    "    obs, _ = env_test.reset()\n",
    "    done = False\n",
    "    torso_z_hist.append(obs[0])\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.as_tensor(obs, dtype = torch.float32, device = 'cpu')\n",
    "            act = policy_net(obs_t)\n",
    "            act += torch.randn_like(act) * noise_std\n",
    "\n",
    "            nobs, rew, term, trunc, _ = env_test.step(act.numpy())\n",
    "\n",
    "            act_hist.append(act.numpy())\n",
    "            torso_z_hist.append(obs[0])\n",
    "\n",
    "            # rew_hist.append(rew)\n",
    "            if truncated:\n",
    "                rew_hist.append(rew)\n",
    "                truncated = False\n",
    "            else:\n",
    "                rew_hist.append(rew_hist[-1]+rew)\n",
    "\n",
    "            done = term or trunc\n",
    "            if not done:\n",
    "                obs = nobs  \n",
    "            else:\n",
    "                obs = env_test.reset()[0]\n",
    "                truncated = True\n",
    "                eps_rew_hist.append(rew_hist[-1])\n",
    "                msg = 'done due to termination' if term else 'done due to truncation'\n",
    "    # print(f'episode {eps:3d} ' + msg)\n",
    "env_test.close()\n",
    "\n",
    "plot_fn(rew_hist, ylabel='instantaneous reward')\n",
    "\n",
    "# Statistical analysis\n",
    "if n_test_eps > 1:\n",
    "    print(f\"Evaluation of {run_number} after {n_test_eps} eval episodes | µ = {mean(eps_rew_hist):5.2f} | σ = {stdev(eps_rew_hist):5.2f}\")\n",
    "\n",
    "else:\n",
    "    # --- plot the z-position of the torso\n",
    "    plot_fn(torso_z_hist, ylabel='torso z level')\n",
    "\n",
    "    # --- unpack the actions and plot\n",
    "    act1_hist, act2_hist, act3_hist, act4_hist, act5_hist, act6_hist = zip(*act_hist)\n",
    "    legends=[]\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(range(len(act1_hist)), act1_hist, color = \"blue\"); legends.append(r\"right thigh $\\tau_{0}$\")\n",
    "    plt.plot(range(len(act2_hist)), act2_hist, color = \"orange\"); legends.append(r\"right leg $\\tau_{1}$\")\n",
    "    plt.plot(range(len(act3_hist)), act3_hist, color = \"green\"); legends.append(r\"right foot $\\tau_{2}$\")\n",
    "    plt.title(\"Right leg torques vs time\")\n",
    "    plt.legend(legends, bbox_to_anchor=[0.5, -0.1], loc='center', ncol=10)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "\n",
    "    legends=[]\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.plot(range(len(act4_hist)), act4_hist, color = \"blue\"); legends.append(r\"left thigh $\\tau_{3}$\")\n",
    "    plt.plot(range(len(act5_hist)), act5_hist, color = \"orange\"); legends.append(r\"left leg $\\tau_{l2}$\")\n",
    "    plt.plot(range(len(act6_hist)), act6_hist, color = \"green\"); legends.append(r\"left foot $\\tau_{l3}$\")\n",
    "    plt.title(\"Left leg torques vs time\")\n",
    "    plt.legend(legends, bbox_to_anchor=[0.5, -0.1], loc='center', ncol=10)\n",
    "    plt.grid(which='both')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fe87d",
   "metadata": {},
   "source": [
    "# SB3 TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0f51c",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3,SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import nav2d        # Have to import the nav2d Python script, else we can't make env\n",
    "import nav2d_testing\n",
    "import numpy as np\n",
    "import os, re, json, time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e037c6",
   "metadata": {},
   "source": [
    "## Parameters & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Time taken to do evaluation - {end-start:5.3f}\")\n",
    "\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the evaluation function:\n",
    "# @timer_decorator\n",
    "def eval_policy(env: gym.Env, \n",
    "         num_evals: int, \n",
    "         model):\n",
    "    # reward list:\n",
    "    eval_rew_hist = []\n",
    "\n",
    "    # for each episode in the num_evals:\n",
    "    for _ in range(num_evals):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # initialize episodic reward:\n",
    "        eval_rew = 0\n",
    "\n",
    "        # while False:\n",
    "        while not done:\n",
    "            # get action and step:\n",
    "            with torch.no_grad():\n",
    "                action, _ = model.predict(obs, deterministic = True)\n",
    "                nobs, reward, term, trunc, _ = env.step(action)\n",
    "                done = term or trunc\n",
    "                \n",
    "                # advance reward:\n",
    "                eval_rew += reward\n",
    "\n",
    "                # advance observation, reset if not:\n",
    "                obs = nobs if not done else env.reset()\n",
    "    \n",
    "        # append:\n",
    "        eval_rew_hist.append(eval_rew)\n",
    "\n",
    "    return np.mean(eval_rew_hist).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c765b27",
   "metadata": {},
   "source": [
    "Environment parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a163139",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_scale= {\n",
    "    \"rew_head_scale\": 2.5,\n",
    "    \"rew_head_approach_scale\": 50.0,\n",
    "    \"rew_dist_scale\": 2.5,\n",
    "    \"rew_dist_approach_scale\": 250.0,\n",
    "    \"rew_goal_scale\": 2_000.0,\n",
    "    \"rew_obst_scale\": -1_000.0\n",
    "}\n",
    "\n",
    "# vectorize environments\n",
    "n_envs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac581c",
   "metadata": {},
   "source": [
    "Policy parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "buffer_size=int(1e6)\n",
    "learning_starts=10_000\n",
    "batch_size=2048 \n",
    "tau=5e-3\n",
    "gamma=0.99\n",
    "train_freq=1\n",
    "gradient_steps=1\n",
    "action_noise=None\n",
    "n_steps=1\n",
    "policy_delay=2\n",
    "target_policy_noise=0.1\n",
    "target_noise_clip=0.2\n",
    "verbose=0\n",
    "tensor_board_log_dir=\"./results/Nav2D_TD3_SB3_tensorboard/\"\n",
    "\n",
    "pi_arch = [256, 256]\n",
    "qf_arch = [256, 256]\n",
    "policy_kwargs=dict(activation_fn=torch.nn.ReLU,\n",
    "                   net_arch=dict(pi=pi_arch, qf=qf_arch))\n",
    "use_custom_policy = False\n",
    "vectorize = False\n",
    "cuda_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452738b",
   "metadata": {},
   "source": [
    "## SB3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5daa26",
   "metadata": {},
   "source": [
    "Create the environments for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53751a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vectorize:\n",
    "    print(\"Making a single environment!\")\n",
    "    # # Make one environment using gymnasium API\n",
    "    env = gym.make(\"Nav2D-v0\", render_mode=\"human\", max_episode_steps=1_000, reward_scale_options=reward_scale)\n",
    "else:\n",
    "    # stable_baselines3 API\n",
    "    print(\"Making dummy vectorized environments!\")\n",
    "    env = make_vec_env(\"Nav2D-v0\", \n",
    "                    n_envs=n_envs,\n",
    "                    env_kwargs={\"max_episode_steps\": 1_000,\n",
    "                                \"reward_scale_options\": reward_scale\n",
    "                                },\n",
    "                    vec_env_cls=DummyVecEnv)\n",
    "        \n",
    "# evaluation environment:\n",
    "eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = 1_000, render_mode = \"human\", is_eval=True)\n",
    "\n",
    "# # wrapper to rescale the action of the environment\n",
    "# action_bounds = np.array([1.0, 0.0001, 1.0], dtype=np.float32)\n",
    "# vec_env = RescaleAction(vec_env, min_action=-action_bounds, max_action=action_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c54cc",
   "metadata": {},
   "source": [
    "Create the training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e878da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom MlpPolicy\n",
    "\n",
    "model = TD3(\"MlpPolicy\", env, \n",
    "            learning_rate=learning_rate,         # lr for all networds - Q-values, Actor, Value function\n",
    "            buffer_size=buffer_size,         # replay buffer size\n",
    "            learning_starts=learning_starts,        # # of data collection step before training\n",
    "            batch_size=batch_size,\n",
    "            tau=tau,                   # polyak update coefficient\n",
    "            gamma=gamma,\n",
    "            train_freq=train_freq,\n",
    "            gradient_steps=gradient_steps, \n",
    "            action_noise=action_noise, \n",
    "            n_steps=n_steps,                  # n-step TD learning\n",
    "            policy_delay=policy_delay,             # the policy and target networks are updated every policy_delay steps\n",
    "            target_policy_noise=target_policy_noise,    # stdev of noise added to target policy\n",
    "            target_noise_clip=target_noise_clip,      # limit of asbsolute value of noise\n",
    "            verbose=verbose,\n",
    "            device=\"cuda\" if cuda_enabled else \"cpu\",\n",
    "            policy_kwargs=policy_kwargs if use_custom_policy else None,\n",
    "            tensorboard_log=tensor_board_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run parameters:\n",
    "number_of_runs = 100\n",
    "steps_per_run = 20_000\n",
    "model_save_freq = int(number_of_runs / 20)\n",
    "\n",
    "# model saving parameters:\n",
    "base_path = os.path.join(os.getcwd(), \"results\", \"Nav2D_TD3_SB3_results\")\n",
    "result_number = f\"result_{len(os.listdir(base_path)):05d}\"\n",
    "results_path = os.path.join(base_path, result_number)\n",
    "\n",
    "# using model.learn approach:\n",
    "for run in tqdm(range(1,number_of_runs+1), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "    # learn every run:\n",
    "    model.learn(total_timesteps = steps_per_run, tb_log_name=f\"{result_number}\",reset_num_timesteps = False)\n",
    "    # model.learn(total_timesteps = steps_per_run, reset_num_timesteps = False)\n",
    "\n",
    "    # save a model once in a while\n",
    "    if run % model_save_freq == 0:\n",
    "        model.save(os.path.join(results_path, f\"run_{run}\"))\n",
    "\n",
    "# save the last model\n",
    "model.save(os.path.join(results_path, f\"run_{run}\"))\n",
    "\n",
    "# close environment when done:\n",
    "env.close()\n",
    "\n",
    "# Save the result-params mapping into a json file\n",
    "trial_to_param_path = os.path.join(base_path,'trial_to_param.json')\n",
    "if os.path.exists(trial_to_param_path):\n",
    "    with open(trial_to_param_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = {result_number: \"\"}\n",
    "\n",
    "hyperparam_codified = f\"{learning_rate}_{buffer_size}_{learning_starts}_{batch_size}_{tau}_{gamma}_\"\n",
    "hyperparam_codified += f\"{train_freq}_{gradient_steps}_{n_steps}_{policy_delay}_{target_policy_noise}_{target_noise_clip}_\"\n",
    "hyperparam_codified += f\"{reward_scale['rew_head_scale']}_{reward_scale['rew_dist_scale']}_{reward_scale['rew_goal_scale']}_{reward_scale['rew_obst_scale']}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "data[result_number] = hyperparam_codified_time\n",
    "\n",
    "with open(trial_to_param_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f2c67",
   "metadata": {},
   "source": [
    "# Optuna Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023dcef",
   "metadata": {},
   "source": [
    "Create the Optuna study with the objective function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_model(model,\n",
    "                n_runs : int = 100,\n",
    "                steps_per_run: int = 25_000,\n",
    "                result_number: str | None = None,\n",
    "                result_path: str | None = None):\n",
    "    # model training parameters\n",
    "    model_save_freq = int(n_runs / 10)\n",
    "\n",
    "    # --- train the model\n",
    "    for run in tqdm(range(1,n_runs+1), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "        # learn every run:\n",
    "        model.learn(total_timesteps = steps_per_run, tb_log_name=f\"{result_number}\",reset_num_timesteps = False)\n",
    "\n",
    "        # save a model once in a while\n",
    "        if run % model_save_freq == 0:\n",
    "            model.save(os.path.join(result_path, f\"run_{run}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c145d",
   "metadata": {},
   "source": [
    "## Optuna Study 1 - Reward scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rew_scale(trial):\n",
    "    rew_head_scale = trial.suggest_float(\"rew_head_scale\", low=0.0, high=5.0, step=0.5)\n",
    "    rew_dist_scale = trial.suggest_float(\"rew_dist_scale\", low=0.0, high=5.0, step=0.5)\n",
    "\n",
    "    reward_scale= {\n",
    "        \"rew_head_scale\": rew_head_scale,\n",
    "        \"rew_dist_scale\": rew_dist_scale,\n",
    "        \"rew_goal_scale\": 2_000,\n",
    "        \"rew_obst_scale\": -100\n",
    "    }\n",
    "\n",
    "    # create the env\n",
    "    env = make_vec_env(\"Nav2D-v0\", \n",
    "                   n_envs=n_envs,\n",
    "                   env_kwargs={\n",
    "                       \"max_episode_steps\": 2_000,\n",
    "                       \"reward_scale_options\": reward_scale\n",
    "                   },\n",
    "                   vec_env_cls=DummyVecEnv)\n",
    "    \n",
    "    # create the model\n",
    "    model = TD3(\"MlpPolicy\", env, \n",
    "            learning_rate=learning_rate,         # lr for all networds - Q-values, Actor, Value function\n",
    "            buffer_size=buffer_size,         # replay buffer size\n",
    "            learning_starts=learning_starts,        # # of data collection step before training\n",
    "            batch_size=batch_size,\n",
    "            tau=tau,                   # polyak update coefficient\n",
    "            gamma=gamma,\n",
    "            train_freq=train_freq,\n",
    "            gradient_steps=gradient_steps, \n",
    "            action_noise=action_noise, \n",
    "            n_steps=n_steps,                  # n-step TD learning\n",
    "            policy_delay=policy_delay,             # the policy and target networks are updated every policy_delay steps\n",
    "            target_policy_noise=target_policy_noise,    # stdev of noise added to target policy\n",
    "            target_noise_clip=target_noise_clip,      # limit of asbsolute value of noise\n",
    "            verbose=verbose,\n",
    "            policy_kwargs=policy_kwargs if use_custom_policy else None,\n",
    "            tensorboard_log=tensor_board_log_dir)\n",
    "\n",
    "    # model saving parameters:\n",
    "    base_path = os.path.join(os.getcwd(), \"results\",\"Nav2D_TD3_SB3_results\")\n",
    "    result_number = f\"result_{len(os.listdir(base_path)):05d}\"\n",
    "    result_path = os.path.join(base_path, result_number)\n",
    "\n",
    "    # --- train the model\n",
    "    train_model(model=model,\n",
    "                n_runs=100, \n",
    "                steps_per_run=20_000,\n",
    "                result_number=result_number,\n",
    "                result_path=result_path)    \n",
    "    env.close()\n",
    "\n",
    "    # --- Save the result-params mapping into a json file\n",
    "    trial_to_param_path = os.path.join(base_path,'trial_to_param.json')\n",
    "    if os.path.exists(trial_to_param_path):\n",
    "        with open(trial_to_param_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = {result_number: \"\"}\n",
    "\n",
    "    hyperparam_codified = f\"{learning_rate}_{buffer_size}_{learning_starts}_{batch_size}_{tau}_{gamma}_\"\n",
    "    hyperparam_codified += f\"{train_freq}_{gradient_steps}_{n_steps}_{policy_delay}_{target_policy_noise}_{target_noise_clip}_\"\n",
    "    hyperparam_codified += f\"{reward_scale['rew_head_scale']}_{reward_scale['rew_dist_scale']}_{reward_scale['rew_goal_scale']}_{reward_scale['rew_obst_scale']}\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "    hyperparam_codified_time = f\"{timestamp}_\" + hyperparam_codified\n",
    "\n",
    "    data[result_number] = hyperparam_codified_time\n",
    "\n",
    "    with open(trial_to_param_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    # --- Evaluate the model after training\n",
    "    n_evals = 20\n",
    "    eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = 1_000, render_mode = \"rgb_array\", is_eval=True)\n",
    "    mean_eval_rew = eval_policy(env=eval_env, num_evals=n_evals, model=model)\n",
    "    eval_env.close()\n",
    "    \n",
    "    return mean_eval_rew    # return the mean of evaluation reward as the maximizing objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeef7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"reward_scale_oct25_2\"\n",
    "study = optuna.create_study(storage=f\"sqlite:///results/{study_name}.db\", study_name=study_name, direction='maximize')\n",
    "study.optimize(objective_rew_scale, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08615561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
