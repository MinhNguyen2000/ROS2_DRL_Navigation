{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537adf07",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of Soft Actor-Critic (SAC) on the custom-developed 2D navigation environment, titled ``Nav2D-v0``. The goal of this implementation is to quantify the performance of SAC in a simple 2D navigational  task, such that it can be used for incremental learning within subsequent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d678bf",
   "metadata": {},
   "source": [
    "# **Imports**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8af0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium related packages:\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# import custom environments and wrappers:\n",
    "import nav2d\n",
    "\n",
    "# import stablebaselines stuff:\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import Monitor, make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# other necessary imports:\n",
    "from tqdm import tqdm\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ef3d4",
   "metadata": {},
   "source": [
    "# **Function Definitions**\n",
    "\n",
    "This section defines the functions required for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e587e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function:\n",
    "def eval(env: gym.Env, \n",
    "         num_evals: int, \n",
    "         model):\n",
    "    # reward list:\n",
    "    eval_rew_hist = []\n",
    "\n",
    "    # for each episode in the num_evals:\n",
    "    for _ in range(num_evals):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # initialize episodic reward:\n",
    "        eval_rew = 0\n",
    "\n",
    "        # while False:\n",
    "        while not done:\n",
    "            # get action and step:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            \n",
    "            # advance reward:\n",
    "            eval_rew += reward\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "    \n",
    "        # append:\n",
    "        eval_rew_hist.append(eval_rew)\n",
    "\n",
    "    return np.mean(eval_rew_hist).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041ba",
   "metadata": {},
   "source": [
    "# **Environment Definition and Hyperparameters**\n",
    "\n",
    "This section defines and verifies the environment, defines the hyperparameters for the model, and creates a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56d3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has the following issues: \n",
      "Using `env.reset(seed=123)` is non-deterministic as the observations are not equivalent.\n"
     ]
    }
   ],
   "source": [
    "# make the environment:\n",
    "env = gym.make(\"Nav2D-v0\")\n",
    "\n",
    "# check the environment:\n",
    "try: \n",
    "    check_env(env.unwrapped)\n",
    "    print(f\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has the following issues: \\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0b3c3",
   "metadata": {},
   "source": [
    "Define hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2d4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters:\n",
    "policy = \"MlpPolicy\"\n",
    "gamma = 0.99\n",
    "learning_rate = 3e-4\n",
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "tau = 5e-3\n",
    "ent_coef = \"auto_0.1\"\n",
    "train_freq = 1\n",
    "learning_starts = int(0)\n",
    "target_update_interval = 1\n",
    "gradient_steps = 4\n",
    "target_entropy = \"auto\"\n",
    "action_noise = None\n",
    "verbose = 0\n",
    "\n",
    "# scaling on rewards:\n",
    "rew_head_scale = 2.5\n",
    "rew_dist_scale = 1.5\n",
    "rew_goal_scale = 10\n",
    "rew_obst_scale = -1000.0\n",
    "\n",
    "# vectorize or nah:\n",
    "vectorize = True\n",
    "n_envs = 8\n",
    "render_mode = \"human\"\n",
    "max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f35158",
   "metadata": {},
   "source": [
    "Make envs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cecac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using a vectorized environment:\n",
    "if vectorize:\n",
    "    # make the vectorized environments:\n",
    "    print(\"making vectorized environments!\")\n",
    "    env = make_vec_env(\"Nav2D-v0\", n_envs = n_envs, env_kwargs = {\"reward_scale_options\" : {\"rew_head_scale\" : rew_head_scale, \n",
    "                                                                                            \"rew_dist_scale\" : rew_dist_scale, \n",
    "                                                                                            \"rew_goal_scale\" : rew_goal_scale, \n",
    "                                                                                            \"rew_obst_scale\" : rew_obst_scale},\n",
    "                                                                  \"max_episode_steps\" : max_episode_steps,\n",
    "                                                                  \"render_mode\" : \"rgb_array\"}, vec_env_cls = DummyVecEnv)\n",
    "else:\n",
    "    # make a single environment:\n",
    "    print(\"making single environment!\")\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                    reward_scale_options = {\"rew_head_scale\" : rew_head_scale, \n",
    "                                            \"rew_dist_scale\" : rew_dist_scale,\n",
    "                                            \"rew_goal_scale\" : rew_goal_scale,\n",
    "                                            \"rew_obst_scale\" : rew_obst_scale}, \n",
    "                    max_episode_steps = max_episode_steps, \n",
    "                    render_mode = render_mode)\n",
    "\n",
    "# evaluation environment:\n",
    "eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = max_episode_steps, render_mode = \"rgb_array\", is_eval = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f4f9",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.sac.sac.SAC'>\n"
     ]
    }
   ],
   "source": [
    "# model creation using SB3:\n",
    "model = SAC(policy = policy, \n",
    "            env = env,\n",
    "            learning_rate = learning_rate,\n",
    "            buffer_size = buffer_size,\n",
    "            batch_size = batch_size,\n",
    "            tau = tau,\n",
    "            ent_coef = ent_coef,\n",
    "            train_freq = train_freq,\n",
    "            learning_starts = learning_starts,\n",
    "            target_update_interval = target_update_interval,\n",
    "            gradient_steps = gradient_steps,\n",
    "            target_entropy = target_entropy,\n",
    "            action_noise = action_noise, \n",
    "            verbose = verbose)\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc19b2",
   "metadata": {},
   "source": [
    "# **Train the model**\n",
    "\n",
    "Using the instantiated SB3 model, train on the ``Nav2D-v0`` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f494140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                      \u001b[0m| 0/1 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4 | required: 255.29 | current: 239.69 | diff: 15.59 | rew_head: -0.22 | rew_dist: -0.16 | total: -0.62                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress: 100%|\u001b[38;2;51;255;0m█████████████████████████████████████████████\u001b[0m| 1/1 [01:42<00:00, 102.70s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run parameters:\n",
    "number_of_runs = 100\n",
    "steps_per_run = 25000\n",
    "num_evals = 10\n",
    "\n",
    "# initialize the total reward:\n",
    "total_reward = []\n",
    "\n",
    "# model saving parameters:\n",
    "base_path = os.path.join(os.getcwd(), \"results/Nav2D_SAC_SB3_results\")\n",
    "results_path = os.path.join(base_path, f\"result_{len(os.listdir(base_path)) + 1}\")\n",
    "os.makedirs(results_path, exist_ok = True)\n",
    "\n",
    "# using model.learn approach:\n",
    "for run in tqdm(range(number_of_runs), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "    # learn every run:\n",
    "    model.learn(total_timesteps = steps_per_run, reset_num_timesteps = False)\n",
    "\n",
    "    # # evaluate and save every 10th run:\n",
    "    # if run % max(int(0.1*number_of_runs), 1) == 0:\n",
    "    #     # after learning:\n",
    "    #     eval_reward = eval(eval_env, num_evals = num_evals, model = model)\n",
    "\n",
    "    #     # append the eval reward to the total reward:\n",
    "    #     total_reward.append(eval_reward)\n",
    "\n",
    "    #     # save the model to this directory:\n",
    "    #     model.save(os.path.join(results_path, f\"run_{run}\"))\n",
    "\n",
    "# close environment when done:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa02600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mujoco is angry:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45918",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2c179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False\n",
    "testing_length = 10\n",
    "\n",
    "if visualize:\n",
    "    # render settings:\n",
    "    width = 1280\n",
    "    height = 1280\n",
    "    default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "    camera_id = 2\n",
    "\n",
    "    DEFAULT_CAMERA = \"overhead_camera\"\n",
    "    ENABLE_FRAME = True\n",
    "    RENDER_EVERY_FRAME = True \n",
    "\n",
    "    # make a single environment:\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                render_mode = \"human\", \n",
    "                width = width, \n",
    "                height = height,\n",
    "                default_camera_config = default_camera_config, \n",
    "                camera_id = camera_id, \n",
    "                max_episode_steps = max_episode_steps, \n",
    "                is_eval = True)\n",
    "\n",
    "    if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "    if ENABLE_FRAME: pyautogui.press('e') \n",
    "    if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "    # for every test episode:\n",
    "    for eps in range(testing_length):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # while not done:\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "            \n",
    "            # render for user:\n",
    "            env.render()\n",
    "\n",
    "    # close when done:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
