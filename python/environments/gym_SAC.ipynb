{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537adf07",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of Soft Actor-Critic (SAC) on the custom-developed 2D navigation environment, titled ``Nav2D-v0``. The goal of this implementation is to quantify the performance of SAC in a simple 2D navigational  task, such that it can be used for incremental learning within subsequent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d678bf",
   "metadata": {},
   "source": [
    "# **Imports**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8af0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium related packages:\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# import custom environments and wrappers:\n",
    "import nav2d\n",
    "\n",
    "# import stablebaselines stuff:\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import Monitor, make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# other necessary imports:\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ef3d4",
   "metadata": {},
   "source": [
    "# **Function Definitions**\n",
    "\n",
    "This section defines the functions required for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e587e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function:\n",
    "def eval(env: gym.Env, \n",
    "         num_evals: int, \n",
    "         model):\n",
    "    # reward list:\n",
    "    eval_rew_hist = []\n",
    "\n",
    "    # for each episode in the num_evals:\n",
    "    for _ in range(num_evals):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # initialize episodic reward:\n",
    "        eval_rew = 0\n",
    "\n",
    "        # while False:\n",
    "        while not done:\n",
    "            # get action and step:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            \n",
    "            # advance reward:\n",
    "            eval_rew += reward\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "    \n",
    "        # append:\n",
    "        eval_rew_hist.append(eval_rew)\n",
    "\n",
    "    return np.mean(eval_rew_hist).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041ba",
   "metadata": {},
   "source": [
    "# **Environment Definition and Hyperparameters**\n",
    "\n",
    "This section defines and verifies the environment, defines the hyperparameters for the model, and creates a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56d3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has the following issues: \n",
      "The `.np_random` is not properly been updated after step.\n"
     ]
    }
   ],
   "source": [
    "# make the environment:\n",
    "env = gym.make(\"Nav2D-v0\")\n",
    "\n",
    "# check the environment:\n",
    "try: \n",
    "    check_env(env.unwrapped)\n",
    "    print(f\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has the following issues: \\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0b3c3",
   "metadata": {},
   "source": [
    "Define hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2d4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters:\n",
    "policy = \"MlpPolicy\"\n",
    "gamma = 0.99\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-5\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 4096\n",
    "tau = 5e-3\n",
    "ent_coef = \"auto_0.1\"\n",
    "train_freq = 1\n",
    "learning_starts = 0\n",
    "target_update_interval = 1\n",
    "gradient_steps = 4\n",
    "target_entropy = -env.action_space.shape[0]\n",
    "action_noise = None\n",
    "verbose = 0\n",
    "\n",
    "# scaling on rewards:\n",
    "rew_head_scale = 2.5\n",
    "rew_head_approach_scale = 50\n",
    "rew_dist_scale = 250.0\n",
    "rew_goal_scale = 5000.0\n",
    "rew_obst_scale = -1000.0\n",
    "\n",
    "# vectorize or nah:\n",
    "vectorize = False\n",
    "n_envs = 4\n",
    "render_mode = \"human\"\n",
    "max_episode_steps = 1000\n",
    "gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f35158",
   "metadata": {},
   "source": [
    "Make envs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cecac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making single environment!\n"
     ]
    }
   ],
   "source": [
    "# if using a vectorized environment:\n",
    "if vectorize:\n",
    "    # make the vectorized environments:\n",
    "    print(\"making vectorized environments!\")\n",
    "    env = make_vec_env(\"Nav2D-v0\", n_envs = n_envs, env_kwargs = {\"reward_scale_options\" : {\"rew_head_scale\" : rew_head_scale, \n",
    "                                                                                            \"rew_head_approach_scale\" : rew_head_approach_scale,\n",
    "                                                                                            \"rew_dist_scale\" : rew_dist_scale, \n",
    "                                                                                            \"rew_goal_scale\" : rew_goal_scale, \n",
    "                                                                                            \"rew_obst_scale\" : rew_obst_scale},\n",
    "                                                                  \"max_episode_steps\" : max_episode_steps,\n",
    "                                                                  \"render_mode\" : \"rgb_array\"}, vec_env_cls = DummyVecEnv)\n",
    "else:\n",
    "    # make a single environment:\n",
    "    print(\"making single environment!\")\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                    reward_scale_options = {\"rew_head_scale\" : rew_head_scale, \n",
    "                                            \"rew_head_approach_scale\" : rew_head_approach_scale,\n",
    "                                            \"rew_dist_scale\" : rew_dist_scale,\n",
    "                                            \"rew_goal_scale\" : rew_goal_scale,\n",
    "                                            \"rew_obst_scale\" : rew_obst_scale}, \n",
    "                    max_episode_steps = max_episode_steps, \n",
    "                    render_mode = render_mode)\n",
    "\n",
    "# evaluation environment:\n",
    "eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = max_episode_steps, render_mode = \"rgb_array\", is_eval = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f4f9",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created model using: cuda as device\n"
     ]
    }
   ],
   "source": [
    "# model creation using SB3:\n",
    "model = SAC(policy = policy, \n",
    "            env = env,\n",
    "            buffer_size = buffer_size,\n",
    "            batch_size = batch_size,\n",
    "            tau = tau,\n",
    "            ent_coef = ent_coef,\n",
    "            train_freq = train_freq,\n",
    "            learning_starts = learning_starts,\n",
    "            target_update_interval = target_update_interval,\n",
    "            gradient_steps = gradient_steps,\n",
    "            target_entropy = target_entropy,\n",
    "            action_noise = action_noise, \n",
    "            verbose = verbose,\n",
    "            device = \"cuda\" if gpu else \"cpu\")\n",
    "\n",
    "model.actor.optimizer = torch.optim.Adam(model.actor.parameters(), lr = actor_lr)\n",
    "model.critic.optimizer = torch.optim.Adam(model.critic.parameters(), lr = critic_lr)\n",
    "\n",
    "print(f\"created model using: {model.device} as device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc19b2",
   "metadata": {},
   "source": [
    "# **Train the model**\n",
    "\n",
    "Using the instantiated SB3 model, train on the ``Nav2D-v0`` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f494140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                     \u001b[0m| 0/10 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ episode 30 | rew_head: 0.0000 | head_diff: 0.00527 | rew_head_approach: 0.2637 | vel: 0.04616 | pos_diff: 0.00000 | rew_approach: 0.0000 | total: 0.21374                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\glfw\\__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ episode 30 | rew_head: 0.0000 | head_diff: 0.00000 | rew_head_approach: 0.0000 | vel: 0.24776 | pos_diff: 0.00000 | rew_approach: 0.0000 | total: -0.04999                                                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                     \u001b[0m| 0/10 [19:24<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ episode 30 | rew_head: 0.0000 | head_diff: 0.00469 | rew_head_approach: 0.2346 | vel: 0.07002 | pos_diff: 0.00000 | rew_approach: 0.0000 | total: 0.18461                                                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# using model.learn approach:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(number_of_runs), ncols = \u001b[32m100\u001b[39m, colour = \u001b[33m\"\u001b[39m\u001b[33m#33FF00\u001b[39m\u001b[33m\"\u001b[39m, desc = \u001b[33m\"\u001b[39m\u001b[33mtraining progress\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# learn every run:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# # evaluate and save every 10th run:\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# if run % max(int(number_of_runs/10), 1) == 0:\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m#     # NOT SURE IF THIS SHOULD BE IN HERE: os.makedirs(results_path, exist_ok = True)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# close environment when done:\u001b[39;00m\n\u001b[32m     31\u001b[39m env.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:313\u001b[39m, in \u001b[36mSAC.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[32m    306\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    312\u001b[39m ) -> SelfSAC:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:354\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[32m    353\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m callback.on_training_end()\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:282\u001b[39m, in \u001b[36mSAC.train\u001b[39m\u001b[34m(self, gradient_steps, batch_size)\u001b[39m\n\u001b[32m    280\u001b[39m min_qf_pi, _ = th.min(q_values_pi, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    281\u001b[39m actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m actor_losses.append(\u001b[43mactor_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28mself\u001b[39m.actor.optimizer.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run parameters:\n",
    "number_of_runs = 10\n",
    "steps_per_run = 25000\n",
    "num_evals = 10\n",
    "\n",
    "# initialize the total reward:\n",
    "total_reward = []\n",
    "\n",
    "# model saving parameters:\n",
    "base_path = os.path.join(os.getcwd(), \"results/Nav2D_SAC_SB3_results\")\n",
    "results_path = os.path.join(base_path, f\"result_{len(os.listdir(base_path)) + 1}\")\n",
    "\n",
    "# using model.learn approach:\n",
    "for run in tqdm(range(number_of_runs), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "    # learn every run:\n",
    "    model.learn(total_timesteps = steps_per_run, reset_num_timesteps = False)\n",
    "\n",
    "    # # evaluate and save every 10th run:\n",
    "    # if run % max(int(number_of_runs/10), 1) == 0:\n",
    "    #     # NOT SURE IF THIS SHOULD BE IN HERE: os.makedirs(results_path, exist_ok = True)\n",
    "    #     # after learning:\n",
    "    #     eval_reward = eval(eval_env, num_evals = num_evals, model = model)\n",
    "\n",
    "    #     # append the eval reward to the total reward:\n",
    "    #     total_reward.append(eval_reward)\n",
    "\n",
    "    #     # save the model to this directory:\n",
    "    #     model.save(os.path.join(results_path, f\"run_{run}\"))\n",
    "\n",
    "# close environment when done:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa02600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mujoco is angry:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45918",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "testing_length = 10\n",
    "\n",
    "if visualize:\n",
    "    # render settings:\n",
    "    width = 1280\n",
    "    height = 1280\n",
    "    default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "    camera_id = 2\n",
    "\n",
    "    DEFAULT_CAMERA = \"overhead_camera\"\n",
    "    ENABLE_FRAME = True\n",
    "    RENDER_EVERY_FRAME = True \n",
    "\n",
    "    # make a single environment:\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                render_mode = \"human\", \n",
    "                width = width, \n",
    "                height = height,\n",
    "                default_camera_config = default_camera_config, \n",
    "                camera_id = camera_id, \n",
    "                max_episode_steps = max_episode_steps, \n",
    "                is_eval = False)\n",
    "\n",
    "    if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "    if ENABLE_FRAME: pyautogui.press('e') \n",
    "    if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "    # for every test episode:\n",
    "    for eps in range(testing_length):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # while not done:\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "            \n",
    "            # render for user:\n",
    "            env.render()\n",
    "\n",
    "    # close when done:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
