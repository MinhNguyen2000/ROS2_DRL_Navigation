{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537adf07",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of Soft Actor-Critic (SAC) on the custom-developed 2D navigation environment, titled ``Nav2D-v0``. The goal of this implementation is to quantify the performance of SAC in a simple 2D navigational  task, such that it can be used for incremental learning within subsequent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d678bf",
   "metadata": {},
   "source": [
    "# **Imports**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8af0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium related packages:\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# import custom environments and wrappers:\n",
    "import nav2d\n",
    "\n",
    "# import stablebaselines stuff:\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import Monitor, make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# other necessary imports:\n",
    "from tqdm import tqdm\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ef3d4",
   "metadata": {},
   "source": [
    "# **Function Definitions**\n",
    "\n",
    "This section defines the functions required for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33e587e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function:\n",
    "def eval(env: gym.Env, \n",
    "         num_evals: int, \n",
    "         model):\n",
    "    # reward list:\n",
    "    eval_rew_hist = []\n",
    "\n",
    "    # for each episode in the num_evals:\n",
    "    for _ in range(num_evals):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # initialize episodic reward:\n",
    "        eval_rew = 0\n",
    "\n",
    "        # while False:\n",
    "        while not done:\n",
    "            # get action and step:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            \n",
    "            # advance reward:\n",
    "            eval_rew += reward\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "    \n",
    "        # append:\n",
    "        eval_rew_hist.append(eval_rew)\n",
    "\n",
    "    return np.mean(eval_rew_hist).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041ba",
   "metadata": {},
   "source": [
    "# **Environment Definition and Hyperparameters**\n",
    "\n",
    "This section defines and verifies the environment, defines the hyperparameters for the model, and creates a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f56d3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has the following issues: 9.33 | diff: 26.56 | rew_head: -0.15 | rew_dist: -0.33 | total: -0.72                                              \n",
      "The `.np_random` is not properly been updated after step.\n"
     ]
    }
   ],
   "source": [
    "# make the environment:\n",
    "env = gym.make(\"Nav2D-v0\")\n",
    "\n",
    "# check the environment:\n",
    "try: \n",
    "    check_env(env.unwrapped)\n",
    "    print(f\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has the following issues: \\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0b3c3",
   "metadata": {},
   "source": [
    "Define hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters:\n",
    "policy = \"MlpPolicy\"\n",
    "gamma = 0.99\n",
    "learning_rate = 3e-4\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 64\n",
    "tau = 5e-3\n",
    "ent_coef = \"auto_0.1\"\n",
    "train_freq = 1\n",
    "learning_starts = int(0)\n",
    "target_update_interval = 1\n",
    "gradient_steps = 4\n",
    "target_entropy = \"auto\"\n",
    "action_noise = None\n",
    "verbose = 0\n",
    "\n",
    "# scaling on rewards:\n",
    "rew_head_scale = 2.5\n",
    "rew_dist_scale = 5.0\n",
    "rew_goal_scale = 2000.0\n",
    "rew_obst_scale = -1000.0\n",
    "\n",
    "# vectorize or nah:\n",
    "vectorize = True\n",
    "n_envs = 64\n",
    "render_mode = \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f35158",
   "metadata": {},
   "source": [
    "Make envs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cecac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using a vectorized environment:\n",
    "if vectorize:\n",
    "    # make the vectorized environments:\n",
    "    env = make_vec_env(\"Nav2D-v0\", n_envs = n_envs, env_kwargs = {\"reward_scale_options\" : {\"rew_head_scale\" : rew_head_scale, \n",
    "                                                                                            \"rew_dist_scale\" : rew_dist_scale, \n",
    "                                                                                            \"rew_goal_scale\" : rew_goal_scale, \n",
    "                                                                                            \"rew_obst_scale\" : rew_obst_scale},\n",
    "                                                                  \"max_episode_steps\" : 1000,\n",
    "                                                                  \"render_mode\" : \"rgb_array\"}, vec_env_cls = DummyVecEnv)\n",
    "else:\n",
    "    # make a single environment:\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                    reward_scale_options = {\"rew_head_scale\" : rew_head_scale, \n",
    "                                            \"rew_dist_scale\" : rew_dist_scale,\n",
    "                                            \"rew_goal_scale\" : rew_goal_scale,\n",
    "                                            \"rew_obst_scale\" : rew_obst_scale}, \n",
    "                    max_episode_steps = 1000, \n",
    "                    render_mode = render_mode)\n",
    "\n",
    "# evaluation environment:\n",
    "eval_env = gym.make(\"Nav2D-v0\", max_episode_steps = 1000, render_mode = \"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f4f9",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "515351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.sac.sac.SAC'>\n"
     ]
    }
   ],
   "source": [
    "# model creation using SB3:\n",
    "model = SAC(policy = policy, \n",
    "            env = env,\n",
    "            learning_rate = learning_rate,\n",
    "            buffer_size = buffer_size,\n",
    "            batch_size = batch_size,\n",
    "            tau = tau,\n",
    "            ent_coef = ent_coef,\n",
    "            train_freq = train_freq,\n",
    "            learning_starts = learning_starts,\n",
    "            target_update_interval = target_update_interval,\n",
    "            gradient_steps = gradient_steps,\n",
    "            target_entropy = target_entropy,\n",
    "            action_noise = action_noise, \n",
    "            verbose = verbose)\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc19b2",
   "metadata": {},
   "source": [
    "# **Train the model**\n",
    "\n",
    "Using the instantiated SB3 model, train on the ``Nav2D-v0`` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f494140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                    \u001b[0m| 0/100 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | required: 297.17 | current: 244.93 | diff: 52.23 | rew_head: -0.73 | rew_dist: -0.97 | total: -1.95                                              \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training progress:   0%|\u001b[38;2;51;255;0m                                                    \u001b[0m| 0/100 [00:02<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# using model.learn approach:\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(number_of_runs), ncols = \u001b[32m100\u001b[39m, colour = \u001b[33m\"\u001b[39m\u001b[33m#33FF00\u001b[39m\u001b[33m\"\u001b[39m, desc = \u001b[33m\"\u001b[39m\u001b[33mtraining progress\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# learn every run:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# evaluate and save every 10th run:\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# after learning:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:313\u001b[39m, in \u001b[36mSAC.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[32m    306\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    312\u001b[39m ) -> SelfSAC:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:354\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[32m    353\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m callback.on_training_end()\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:255\u001b[39m, in \u001b[36mSAC.train\u001b[39m\u001b[34m(self, gradient_steps, batch_size)\u001b[39m\n\u001b[32m    253\u001b[39m next_actions, next_log_prob = \u001b[38;5;28mself\u001b[39m.actor.action_log_prob(replay_data.next_observations)\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Compute the next Q values: min over all critics targets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m next_q_values = th.cat(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_actions\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m    256\u001b[39m next_q_values, _ = th.min(next_q_values, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# add entropy term\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:977\u001b[39m, in \u001b[36mContinuousCritic.forward\u001b[39m\u001b[34m(self, obs, actions)\u001b[39m\n\u001b[32m    975\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.extract_features(obs, \u001b[38;5;28mself\u001b[39m.features_extractor)\n\u001b[32m    976\u001b[39m qvalue_input = th.cat([features, actions], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_networks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:977\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    975\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.extract_features(obs, \u001b[38;5;28mself\u001b[39m.features_extractor)\n\u001b[32m    976\u001b[39m qvalue_input = th.cat([features, actions], dim=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqvalue_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m q_net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.q_networks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run parameters:\n",
    "number_of_runs = 100\n",
    "steps_per_run = 25000\n",
    "num_evals = 10\n",
    "\n",
    "# initialize the total reward:\n",
    "total_reward = []\n",
    "\n",
    "# model saving parameters:\n",
    "base_path = os.path.join(os.getcwd(), \"results/Nav2D_SAC_SB3_results\")\n",
    "results_path = os.path.join(base_path, f\"result_{len(os.listdir(base_path)) + 1}\")\n",
    "os.makedirs(results_path, exist_ok = True)\n",
    "\n",
    "# using model.learn approach:\n",
    "for run in tqdm(range(number_of_runs), ncols = 100, colour = \"#33FF00\", desc = \"training progress\"):\n",
    "    # learn every run:\n",
    "    model.learn(total_timesteps = steps_per_run, reset_num_timesteps = False)\n",
    "\n",
    "    # evaluate and save every 10th run:\n",
    "    if run % 10 == 0:\n",
    "        # after learning:\n",
    "        eval_reward = eval(eval_env, num_evals = num_evals, model = model)\n",
    "\n",
    "        # append the eval reward to the total reward:\n",
    "        total_reward.append(eval_reward)\n",
    "\n",
    "        # save the model to this directory:\n",
    "        model.save(os.path.join(results_path, f\"run_{run+1}\"))\n",
    "\n",
    "# close environment when done:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa02600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mujoco is angry:\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45918",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 | required: 133.43 | current: 241.57 | diff: 108.14 | rew_head: -0.60 | rew_dist: -0.34 | total: -1.19                                             \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 19 | required: 338.43 | current: 346.73 | diff: 8.30 | rew_head: -0.05 | rew_dist: -0.02 | total: -0.32                                               \r"
     ]
    }
   ],
   "source": [
    "visualize = True\n",
    "testing_length = 10\n",
    "\n",
    "if visualize:\n",
    "    # render settings:\n",
    "    width = 1280\n",
    "    height = 1280\n",
    "    default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "    camera_id = 2\n",
    "\n",
    "    DEFAULT_CAMERA = \"overhead_camera\"\n",
    "    ENABLE_FRAME = True\n",
    "    RENDER_EVERY_FRAME = True \n",
    "\n",
    "    # make a single environment:\n",
    "    env = gym.make(\"Nav2D-v0\", \n",
    "                render_mode = \"human\", \n",
    "                width = width, \n",
    "                height = height,\n",
    "                default_camera_config = default_camera_config, \n",
    "                camera_id = camera_id, \n",
    "                max_episode_steps = 1000)\n",
    "\n",
    "    if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "    if ENABLE_FRAME: pyautogui.press('e') \n",
    "    if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "    # for every test episode:\n",
    "    for eps in range(testing_length):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # while not done:\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic = True)\n",
    "            nobs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "\n",
    "            # advance observation, reset if not:\n",
    "            obs = nobs if not done else env.reset()\n",
    "            \n",
    "            # render for user:\n",
    "            env.render()\n",
    "\n",
    "    # close when done:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
