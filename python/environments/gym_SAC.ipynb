{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537adf07",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook serves as an implementation of Soft Actor-Critic (SAC) on the custom-developed 2D navigation environment, titled ``Nav2D-v0``. The goal of this implementation is to quantify the performance of SAC in a simple 2D navigational  task, such that it can be used for incremental learning within subsequent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d678bf",
   "metadata": {},
   "source": [
    "# **Imports**\n",
    "\n",
    "This section imports the necessary packages for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8af0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium related packages:\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "# import custom environments and wrappers:\n",
    "import nav2d\n",
    "\n",
    "# import stablebaselines stuff:\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import Monitor\n",
    "\n",
    "# other necessary imports:\n",
    "from tqdm import tqdm\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758041ba",
   "metadata": {},
   "source": [
    "# **Environment Definition and Hyperparameters**\n",
    "\n",
    "This section defines and verifies the environment, defines the hyperparameters for the model, and creates a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56d3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has the following issues: \n",
      "The first element returned by `env.reset()` is not within the observation space.\n"
     ]
    }
   ],
   "source": [
    "# make the environment:\n",
    "env = gym.make(\"Nav2D-v0\")\n",
    "\n",
    "# check the environment:\n",
    "try: \n",
    "    check_env(env.unwrapped)\n",
    "    print(f\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has the following issues: \\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0b3c3",
   "metadata": {},
   "source": [
    "Define hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2d4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an environment:\n",
    "env = gym.make(\"Nav2D-v0\", max_episode_steps = 1000, render_mode = \"human\")\n",
    "\n",
    "# hyperparameters:\n",
    "policy = \"MlpPolicy\"\n",
    "gamma = 0.99\n",
    "learning_rate = 3e-4\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 64\n",
    "tau = 5e-3\n",
    "ent_coef = \"auto_0.1\"\n",
    "train_freq = 1\n",
    "learning_starts = int(0)\n",
    "target_update_interval = 1\n",
    "gradient_steps = 4\n",
    "target_entropy = \"auto\"\n",
    "action_noise = None\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f4f9",
   "metadata": {},
   "source": [
    "Create model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# model creation using SB3:\n",
    "model = SAC(policy = policy, \n",
    "            env = env,\n",
    "            learning_rate = learning_rate,\n",
    "            buffer_size = buffer_size,\n",
    "            batch_size = batch_size,\n",
    "            tau = tau,\n",
    "            ent_coef = ent_coef,\n",
    "            train_freq = train_freq,\n",
    "            learning_starts = learning_starts,\n",
    "            target_update_interval = target_update_interval,\n",
    "            gradient_steps = gradient_steps,\n",
    "            target_entropy = target_entropy,\n",
    "            action_noise = action_noise, \n",
    "            verbose = verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc19b2",
   "metadata": {},
   "source": [
    "# **Train the model**\n",
    "\n",
    "Using the instantiated SB3 model, train on the ``Nav2D-v0`` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f494140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current: 29.08 | desired: 45.98 | rew_heading: -0.91 | rew_dist: -0.96 | total: -2.87                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# using model.learn approach:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:313\u001b[39m, in \u001b[36mSAC.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[32m    306\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    312\u001b[39m ) -> SelfSAC:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:354\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[32m    353\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m callback.on_training_end()\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtidd2\\Desktop\\ROS2_DRL_Navigation\\.venv\\Lib\\site-packages\\stable_baselines3\\sac\\sac.py:281\u001b[39m, in \u001b[36mSAC.train\u001b[39m\u001b[34m(self, gradient_steps, batch_size)\u001b[39m\n\u001b[32m    279\u001b[39m q_values_pi = th.cat(\u001b[38;5;28mself\u001b[39m.critic(replay_data.observations, actions_pi), dim=\u001b[32m1\u001b[39m)\n\u001b[32m    280\u001b[39m min_qf_pi, _ = th.min(q_values_pi, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m actor_loss = \u001b[43m(\u001b[49m\u001b[43ment_coef\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_qf_pi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m actor_losses.append(actor_loss.item())\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# using model.learn approach:\n",
    "model.learn(total_timesteps = 25000, log_interval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45918",
   "metadata": {},
   "source": [
    "# **Visualization**\n",
    "\n",
    "This section visualizes the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # render settings:\n",
    "# width = 1280\n",
    "# height = 1280\n",
    "# default_camera_config = {\"azimuth\" : 90.0, \"elevation\" : -90.0, \"distance\" : 3, \"lookat\" : [0.0, 0.0, 0.0]}\n",
    "# camera_id = 2\n",
    "\n",
    "# DEFAULT_CAMERA = \"overhead_camera\"\n",
    "# ENABLE_FRAME = True\n",
    "# RENDER_EVERY_FRAME = True \n",
    "\n",
    "# # make a single environment:\n",
    "# env = gym.make(\"Nav2D-v0\", \n",
    "#                render_mode = \"human\", \n",
    "#                width = width, \n",
    "#                height = height,\n",
    "#                default_camera_config = default_camera_config, \n",
    "#                camera_id = camera_id, \n",
    "#                max_episode_steps = 500)\n",
    "\n",
    "# env = RescaleAction(env, min_action = -action_bounds, max_action = action_bounds)\n",
    "\n",
    "# if DEFAULT_CAMERA==\"overhead_camera\": pyautogui.press('tab')\n",
    "# if ENABLE_FRAME: pyautogui.press('e') \n",
    "# if not RENDER_EVERY_FRAME: pyautogui.press('d') \n",
    "\n",
    "# # for every test episode:\n",
    "# for eps in range(10):\n",
    "#     obs, _ = env.reset()\n",
    "#     done = False\n",
    "\n",
    "#     # while not done:\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic = True)\n",
    "#         nobs, reward, term, trunc, _ = env.step(action)\n",
    "#         done = term or trunc\n",
    "\n",
    "#         # advance observation, reset if not:\n",
    "#         obs = nobs if not done else env.reset()\n",
    "        \n",
    "#         # render for user:\n",
    "#         env.render()\n",
    "\n",
    "# # close when done:\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
